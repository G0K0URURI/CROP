{
    "BC_initial_step": 0.0,
    "actor_lr": 0.0001,
    "actor_mlp_hidden_size": 256,
    "alpha_lr": 0.0001,
    "best_env_nums": 5,
    "beta": 0.01,
    "critic_lr": 0.0003,
    "critic_mlp_hidden_size": 512,
    "critic_update_pre_step": 2,
    "domain": "walker2d",
    "env": "walker2d-random-v2",
    "env_lr": 0.001,
    "env_nums": 7,
    "env_path": "env",
    "fake_batch_size": 256,
    "log_alpha": 0.0,
    "max_epochs_since_update": 30,
    "max_logstd": 0.25,
    "min_logstd": -5,
    "model_batch_size": 256,
    "model_epoch": 15000.0,
    "model_retain_epochs": 5,
    "offline_batch_size": 256,
    "random_action_num": 10,
    "random_roll_out": false,
    "reward_mlp_hidden_size": [
        200,
        200,
        200,
        200
    ],
    "reward_path": "reward-beta.01-seed0",
    "reward_range": 1.0,
    "reward_weight_decays": [
        5e-05,
        5e-05,
        5e-05,
        5e-05,
        5e-05
    ],
    "rl_path": "rl-beta.01-roll10-seed0",
    "rl_step": 3000000.0,
    "roll_out_batch_size": 50000,
    "roll_out_freq": 1000,
    "roll_out_length": 10,
    "save_path": "walker2d-r",
    "seed": 0,
    "soft_tau": 0.005,
    "state_mlp_hidden_size": [
        200,
        200,
        200,
        200
    ],
    "state_path": "state-seed0",
    "state_weight_decays": [
        5e-05,
        5e-05,
        5e-05,
        5e-05,
        5e-05
    ],
    "target_entropy": null,
    "train_reward": true,
    "train_rl": true,
    "train_state": true,
    "valid_size": 0.01
}